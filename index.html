<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Junyang Wang</title>

  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="css/small-business.css" rel="stylesheet">
  <link rel="stylesheet" href="css/all.css" />
  <link rel="stylesheet" href="css/academicons.css" />
  <style type="text/css">
    .content {
      margin-top: 0;
      margin-bottom: 0
    }
  </style>
  <meta http-equiv="Content-Type" content="text/html; charset=gbk">
  <meta name="google-site-verification" content="qEih9m0y-6X0QuisQYfHSxOvkW-o5Q3dfxuQ5Z4JtGA" />

</head>

<body>
  <div class="container">
    <div class="row my-4">
      <div class="col-lg-3 text-center">
        <img class="img-fluid rounded img-responsive mx-auto d-block" style="width:80%" src="img/me.jpg" alt="">
      </div>
      <div class="col-lg-9">
        <h1>Junyang Wang&nbsp;<small>(王君阳)</small></h1>        
        <p>Email: junyangwang@bjtu.edu.cn; junyangwang287@gmail.com</p>
        <p>I am a research intern at <span style="color:#4169E1"><a href="https://tongyi.aliyun.com/">Institute for Intelligent Computing</a></span> of Alibaba Group.</p>
        <p>I am a Ph.D candidate in the <span style="color:#4169E1"><a href="http://www.bjtu.edu.cn/">School of Computer and Information Technology, BJTU</a></span> and work with <span style="color:#4169E1"><a href="http://faculty.bjtu.edu.cn/9129/">Prof. Jitao Sang</a></span>.</p>
        <p>My current research content is Multi-modal Large Language Model (MLLMs), including MLLMs hallucination and MLLM-based agent. In addition, I have also studied Vision-Language Pre-training (VLP) and social fairness in computer vision.</p>
      </div>
    </div>

    <div class=" my-4 text-left">
      <h2>Recent News</h2>
      <p> * [03.2025] Our paper <span style="color:#4169E1"><a href="https://arxiv.org/abs/2502.14282">PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC
      </a></span> has been accepted by <strong>ICLR 2025 Workshop</strong></p>
      
      <p> * [11.2024] Our github repositoriy <span style="color:#4169E1"><a href="https://github.com/X-PLUG/MobileAgent">Mobile-Agent
      </a></span> has gained <strong>3k stars</strong>.</p>

      <p> * [09.2024] Our paper <span style="color:#4169E1"><a href="https://arxiv.org/abs/2406.01014">Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration
      </a></span> has been accepted by <strong>NeurIPS 2024</strong>.</p>

      <p> * [07.2024] Our work <span style="color:#4169E1"><a href="https://github.com/X-PLUG/MobileAgent">Mobile-Agent
      </a></span> won the <strong>best demo award</strong> at the The 23rd China National Conference on Computational Linguistics (CCL 2024).</p>
      
      <p> * [03.2024] Our paper <span style="color:#4169E1"><a href="https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent">Mobile-Agent
      </a></span> has been accepted by <strong>ICLR 2024 Workshop</strong></p>

      <p> * [07.2023] Our paper <span style="color:#4169E1"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Improved_Visual_Fine-tuning_with_Natural_Language_Supervision_ICCV_2023_paper.html">Improved Visual Fine-tuning with Natural Language Supervision
      </a></span> has been accepted by <strong>ICCV 2023 Oral</strong>.</p>

      <p> * [04.2023] Our paper <span style="color:#4169E1"><a href="https://www.ijcai.org/proceedings/2023/481">From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping
      </a></span> has been accepted by <strong>IJCAI 2023</strong>.</p>

      <p> * [10.2022] I joined Intelligent Computing of Alibaba Group, Ltd as a research intern.</p>

      <p> * [06.2022] Our paper <span style="color:#4169E1"><a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548396">Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models
      </a></span> has been accepted by <strong>MM 2022</strong>.</p>

    </div>

    <div class=" my-4 text-left">

      <h2>Publications</h2>
      <p> See <a href="https://scholar.google.com/citations?user=m4ro0NsAAAAJ">Google scholar</a></p> 
      <ul>
        
      <li><b><a href="https://arxiv.org/abs/2406.01014">Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration</a></b>
        <p class='content'> <strong>Junyang Wang</strong>, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, Jitao Sang.
        </p>
        <p>Annual Conference on Neural Information Processing Systems. <strong>NeurIPS 2024</strong> (CCF-A).</p>
      </li>
        
      <li><b><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Improved_Visual_Fine-tuning_with_Natural_Language_Supervision_ICCV_2023_paper.html">Improved Visual Fine-tuning with Natural Language Supervision</a></b>
        <p class='content'> <strong>Junyang Wang</strong>, Yuanhong Xu, Juhua Hu, Jitao Sang, Qi Qian.
        </p>
        <p>IEEE/CVF International Conference on Computer Vision. <strong>ICCV 2023 Oral</strong> (CCF-A).</p>
      </li>
        
      <li><b><a href="https://www.ijcai.org/proceedings/2023/481">From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping</a></b>
        <p class='content'> <strong>Junyang Wang</strong>, Ming Yan, Yi Zhang, Jitao Sang.
        </p>
        <p>International Joint Conference on Artificial Intelligence. <strong>IJCAI 2023</strong> (CCF-A).</p>
      </li>
        
      <li><b><a href="https://arxiv.org/abs/2401.16158">Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception</a></b>
        <p class='content'> <strong>Junyang Wang</strong>, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, Jitao Sang.
        </p>
        <p>ICLR 2024 Workshop on Large Language Model (LLM) Agents. <strong>ICLR 2024 Workshop</strong>.</p>
      </li>
        
      <li><b><a href="https://dl.acm.org/doi/10.1145/3394171.3413772">Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models</a></b>
        <p class='content'> Yi Zhang, <strong>Junyang Wang</strong>, Jitao Sang.
        </p>
        <p>ACM International Conference on Multimedia. <strong>ACM MM 2022</strong> (CCF-A).</p>
      </li>
        
      <li><b><a href="https://dl.acm.org/doi/10.1145/3581783.3612317">Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features</a></b>
        <p class='content'> Yi Zhang, Jitao Sang, <strong>Junyang Wang</strong>, Dongmei Jiang, Yaowei Wang.
        </p>
        <p>ACM International Conference on Multimedia. <strong>ACM MM 2023</strong> (CCF-A).</p>
      </li>
        
      <li><b><a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612665">mPLUG-Octopus: The Versatile Assistant Empowered by A Modularized End-to-End Multimodal LLM</a></b>
        <p class='content'> Qinghao Ye, Haiyang Xu, Ming Yan, Chenlin Zhao, <strong>Junyang Wang</strong>, Xiaoshan Yang, Ji Zhang, Fei Huang, Jitao Sang, Changsheng Xu.
        </p>
        <p>ACM International Conference on Multimedia. <strong>ACM MM 2023</strong> (CCF-A).</p>
      </li>

      <li><b><a href="https://arxiv.org/abs/2401.16158">Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception</a></b>
        <p class='content'> Haowei Liu, Xi Zhang, Haiyang Xu, Yuyang Wanyan, <strong>Junyang Wang</strong>, Ming Yan, Ji Zhang, Chunfeng Yuan, Changsheng Xu, Weiming Hu, Fei Huang.
        </p>
        <p>ICLR 2025 Workshop on LLM Reason and Plan. <strong>ICLR 2025 Workshop</strong>.</p>
      </li>
        
      </ul>
    </div>
    
    <div class=" my-4 text-left">
      <h2>Experience/Education</h2>
      <ul>
        <li> Research Intern, <span style="color:#4169E1"><a href="https://tongyi.aliyun.com/">Institute for Intelligent Computing</a></span> of Alibaba Group. 2022.10 - Now</li>
        <li> Ph.D (computer science), <span style="color:#4169E1"><a href="https://www.bjtu.edu.cn/">Beijing Jiaotong University
        </a></span>. 2023.9 - Now </li>
        <li> M.S (computer science), <span style="color:#4169E1"><a href="https://www.bjtu.edu.cn/">Beijing Jiaotong University
        </a></span>. 2021.9 - 2023.6 </li>
        <li> B.S (computer science), <span style="color:#4169E1"><a href="https://www.bjtu.edu.cn/">Beijing Jiaotong University
        </a></span>. 2017.9 - 2021.6 </li>
      </ul>
    </div>

  <footer class="py-1 bg-dark">
    <div class="container">
      <p class="m-0 text-center text-white">Copyright &copy; Junyang Wang</p>
    </div>
  </footer>

  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

</body>

</html>
